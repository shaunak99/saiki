# describes the mcp servers to use
mcpServers:
  filesystem:
    type: stdio
    command: npx
    args:
      - -y
      - "@modelcontextprotocol/server-filesystem"
      - .
  playwright:
    type: stdio
    command: npx
    args:
      - -y
      - "@playwright/mcp@latest"
  # hf:
  #   type: stdio
  #   command: npx
  #   args:
  #     - -y
  #     - "@llmindset/mcp-hfspace"

# System prompt configuration - defines the agent's behavior and instructions
systemPrompt:
  contributors:
    - id: primary
      type: static
      priority: 0
      content: |
        You are a helpful AI assistant with access to tools.
        Use these tools when appropriate to answer user queries.
        You can use multiple tools in sequence to solve complex problems.
        After each tool result, determine if you need more information or can provide a final answer.
    - id: dateTime
      type: dynamic
      priority: 10
      source: dateTime
      enabled: true

# Optional greeting shown at chat start (UI can consume this)
greeting: "Hi! I‚Äôm Dexto ‚Äî how can I help today?"

# # describes the llm configuration
llm:
  provider: openai
  model: gpt-5-mini
  apiKey: $OPENAI_API_KEY

# Storage configuration - uses a two-tier architecture: cache (fast, ephemeral) and database (persistent, reliable)
# Memory cache with file-based database (good for development with persistence)
storage:
  cache:
    type: in-memory
  database:
    type: sqlite
    # path: ./data/dexto.db

toolConfirmation:
  mode: event-based
  timeout: 120000
  allowedToolsStorage: memory

# Internal resources configuration - manages file system access and blob storage
# NOTE: Blob storage capacity and backend settings are in the 'blobStorage' section below
internalResources:
  enabled: true
  resources:
    # - type: filesystem
    #   paths: ["."]
    #   maxFiles: 1000
    #   maxDepth: 10
    #   includeHidden: false
    #   includeExtensions: [".txt", ".md", ".json", ".yaml", ".yml", ".js", ".ts", ".py", ".html", ".css"]
    - type: blob  # Enables blob resource provider (storage settings in blobStorage section)

# Starter prompts - predefined prompts that appear as clickable buttons in the WebUI
starterPrompts:
  - id: quick-start
    title: "Quick Start Guide"
    description: "Learn the basics and see what you can do"
    prompt: "I'd like to get started quickly. Can you show me a few examples of what you can do and help me understand how to work with you?"
    category: learning
    icon: "üìö"
    priority: 9
  - id: tool-demo
    title: "Tool Demonstration"
    description: "See the tools in action with practical examples"
    prompt: "I'd like to see your tools in action. Can you pick one of your most interesting tools and demonstrate it with a practical example? Show me what it can do and how it works."
    category: tools
    icon: "‚ö°"
    priority: 5
  - id: snake-game
    title: "Create Snake Game"
    description: "Build a fun interactive game with HTML, CSS, and JavaScript"
    prompt: "Create a snake game in a new directory with HTML, CSS, and JavaScript, then open it in the browser for me to play."
    category: coding
    icon: "üêç"
    priority: 4
  - id: connect-tools
    title: "Connect New Tools"
    description: "Browse and add MCP servers to extend capabilities"
    prompt: "I want to connect new tools to expand my capabilities. Can you help me understand what MCP servers are available and how to add them?"
    category: tools
    icon: "üîß"
    priority: 3

# Blob storage configuration - infrastructure-level storage for large files
# This section controls the actual blob backend, size limits, and cleanup policies
blobStorage:
  type: local  # 'local', 's3', 'gcs', 'azure' (s3, gcs, azure coming in Phase 3)
  maxBlobSize: 52428800     # 50MB per blob
  maxTotalSize: 1073741824  # 1GB total storage
  cleanupAfterDays: 30      # Auto-cleanup blobs older than 30 days
  # storePath: ~/.dexto/blobs (defaults to context-aware path for local backend)
  
  # Future S3 configuration example:
  # type: s3
  # bucket: my-dexto-blobs
  # region: us-west-2
  # accessKeyId: ${AWS_ACCESS_KEY_ID}     # Optional, uses AWS SDK credential chain
  # secretAccessKey: ${AWS_SECRET_ACCESS_KEY}

## To use Google Gemini, replace the LLM section with Google Gemini configuration below
## Similar for anthropic/groq/etc.
# llm:
#   provider: google
#   model: gemini-2.0-flash
#   apiKey: $GOOGLE_GENERATIVE_AI_API_KEY
